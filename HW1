//Problem #1:

function GOAL-BASED-AGENT(percept) returns an action
    state <- UPDATE-STATE(state, percept)
    if goal is achieved in state then return STOP
    if plan is empty then
        plan <- FORMULATE-PLAN(state, goal, model)
        if plan is empty then return STOP
    action <- first action of plan
    plan <- rest of plan
    return action

function UPDATE-STATE(state, percept) returns a state
    return percept

function FORMULATE-PLAN(state, goal, model) returns a plan
    // This function formulates a plan to achieve the given goal from the current state using the provided model
    return BASIC-PLAN(state, goal, model)

function BASIC-PLAN(state, goal, model) returns a plan
    // This function uses a basic planning algorithm to formulate a plan
    frontier <- a queue data structure with state as the only element
    explored <- an empty set
    while frontier is not empty do
        state <- POP(frontier)
        if goal is achieved in state then return RECONSTRUCT-PATH(state)
        add state to explored
        for each action in POSSIBLE-ACTIONS(state) do
            child <- RESULT(state, action, model)
            if child is not in explored or frontier then
                add child to frontier
    return failure

function RECONSTRUCT-PATH(state) returns a sequence of actions
    // This function reconstructs the sequence of actions leading to the given state
    sequence <- an empty sequence
    while state has a parent do
        add action from state to sequence
        state <- parent of state
    return reverse(sequence)




//Problem #2:


a) State space:
The state space consists of all possible configurations of the two friends in different cities on the map.

b) Initial state:
The initial state is the configuration where each friend is in their respective starting city.

c) Goal state/goal test:
The goal state is any configuration where both friends are in the same city, indicating that they have met.

d) Transition model/successor function:
The transition model defines how the state changes when an action is taken. In this case, the successor function generates all possible states that can be reached from the current state by moving each friend to a neighboring city.

e) Action cost function:
The action cost function assigns a cost to each action, which is equal to the road distance between the current city and the neighboring city being moved to.


With these definitions, we can apply search algorithms such as breadth-first search, depth-first search, A*, etc., to find the optimal path for the two friends to meet as quickly as possible.



//Problem #3:


from search import PriorityQueue, memoize, Node

def best_first_graph_search(problem, f):
    """Search the nodes with the lowest f scores first.
    You specify the function f(node) that you want to minimize; for example,
    if f is a heuristic estimate to the goal, then we have greedy best
    first search; if f is node.depth then we have breadth-first search.
    There is a subtlety: the line "f = memoize(f, 'f')" means that the f
    values will be cached on the nodes as they are computed. So after doing
    a best first search you can examine the f values of the path returned."""
    
    # we use these two variables at the time of visualisations
    iterations = 0
    all_node_colors = []
    node_colors = dict(initial_node_colors)
    
    f = memoize(f, 'f')
    node = Node(problem.initial)
    
    node_colors[node.state] = "red"
    iterations += 1
    all_node_colors.append(dict(node_colors))
    
    if problem.goal_test(node.state):
        node_colors[node.state] = "green"
        iterations += 1
        all_node_colors.append(dict(node_colors))
        return(iterations, all_node_colors, node)
    
    frontier = PriorityQueue(min, f)
    frontier.append(node)
    
    node_colors[node.state] = "orange"
    iterations += 1
    all_node_colors.append(dict(node_colors))
    
    explored = set()
    while frontier:
        node = frontier.pop()
        
        node_colors[node.state] = "red"
        iterations += 1
        all_node_colors.append(dict(node_colors))
        
        if problem.goal_test(node.state):
            node_colors[node.state] = "green"
            iterations += 1
            all_node_colors.append(dict(node_colors))
            return(iterations, all_node_colors, node)
        
        explored.add(node.state)
        for child in node.expand(problem):
            if child.state not in explored and child not in frontier:
                frontier.append(child)
                node_colors[child.state] = "orange"
                iterations += 1
                all_node_colors.append(dict(node_colors))
            elif child in frontier:
                incumbent = frontier[child]
                if f(child) < f(incumbent):
                    del frontier[incumbent]
                    frontier.append(child)
                    node_colors[child.state] = "orange"
                    iterations += 1
                    all_node_colors.append(dict(node_colors))

        node_colors[node.state] = "gray"
        iterations += 1
        all_node_colors.append(dict(node_colors))
    return None

def uniform_cost_search(start_state, goal_states, graph, edge_costs):
    """Uniform Cost Search"""
    class GraphProblem:
        def __init__(self, initial, goal, graph, costs):
            self.initial = initial
            self.goal = goal
            self.graph = graph
            self.costs = costs

        def actions(self, state):
            return list(self.graph[state].keys())

        def result(self, state, action):
            return action

        def path_cost(self, cost_so_far, state1, action, state2):
            return cost_so_far + self.costs[(state1, state2)]

        def goal_test(self, state):
            return state in self.goal
    
    problem = GraphProblem(start_state, goal_states, graph, edge_costs)
    iterations, all_node_colors, node = best_first_graph_search(problem, lambda node: node.path_cost)
    return iterations, all_node_colors, node

# Example usage:
graph = {
    0: {1: 2, 2: 4},
    1: {3: 5, 4: 2},
    2: {4: 1, 5: 3},
    3: {6: 7},
    4: {6: 6},
    5: {7: 10},
    6: {7: 2},
    7: {}
}

edge_costs = {
    (0, 1): 2,
    (0, 2): 4,
    (1, 3): 5,
    (1, 4): 2,
    (2, 4): 1,
    (2, 5): 3,
    (3, 6): 7,
    (4, 6): 6,
    (5, 7): 10,
    (6, 7): 2
}

start_state = 0
goal_states = [7]

iterations, all_node_colors, node = uniform_cost_search(start_state, goal_states, graph, edge_costs)
if iterations is not None:
    print("Minimum cost to reach goal state:", node.path_cost)
else:
    print("No path to goal state found.")

